{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsaO1wCAq971",
        "outputId": "81e612e8-1a53-4828-ec9c-4c6910636020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prog.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile prog.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cusolverDn.h>\n",
        "\n",
        "int main() {\n",
        "    const int n = 3; // size of the matrix\n",
        "    const int nrhs = 1; // number of right-hand sides\n",
        "\n",
        "    // example matrix - macierz zapisujemy {[0,0], [0,1], [0,2], [1,0], [1,1], [1,2] ,[2,0], [2,1], [2,2]}  -kolumnami\n",
        "    float A[n*n] = {2, 4, -2, 1, -6, 7, 1, 0, 2}; \n",
        "    float B[n*nrhs] = {5,-2,9};\n",
        "\n",
        "    // allocate memory on the device\n",
        "    float *dA, *dB, *dX;\n",
        "    int *dipiv; // pivoting information\n",
        "\n",
        "    cudaMalloc((void**)&dA, n * n * sizeof(float));\n",
        "    cudaMalloc((void**)&dB, n * nrhs * sizeof(float));\n",
        "    cudaMalloc((void**)&dX, n * nrhs * sizeof(float));\n",
        "    cudaMalloc((void**)&dipiv, n * sizeof(int));\n",
        "\n",
        "    // copy input matrices to the device\n",
        "    cudaMemcpy(dA, A, n * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dB, B, n * nrhs * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // create cusolverDn handle\n",
        "    cusolverDnHandle_t handle;\n",
        "    cusolverDnCreate(&handle);\n",
        "\n",
        "    // factorize and solve the linear system\n",
        "    int niter, dinfo;\n",
        "    size_t workspace_size;\n",
        "    cusolverStatus_t status = cusolverDnSSgesv_bufferSize(handle, n, nrhs, dA, n, dipiv, dB, n, dX, n, NULL, &workspace_size);\n",
        "\n",
        "    if (status != CUSOLVER_STATUS_SUCCESS) {\n",
        "        printf(\"Error querying workspace size: %d\\n\", status);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    status = cusolverDnSSgesv(handle, n, nrhs, dA, n, dipiv, dB, n, dX, n, NULL, workspace_size, &niter, &dinfo);\n",
        "\n",
        "    printf(\"%d %long %d %d\\n\", status, workspace_size, dinfo, niter);\n",
        "    if (status != CUSOLVER_STATUS_SUCCESS) {\n",
        "        printf(\"Error solving linear system: %d\\n\", status);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    if (dinfo != 0) {\n",
        "        printf(\"Error solving linear system: dinfo = %d\\n\", dinfo);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // copy the solution back to the host\n",
        "    float X[n*nrhs];\n",
        "    cudaMemcpy(X, dX, n * nrhs * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print the solution\n",
        "    printf(\"Solution:\\n\");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%f\\n\", X[i]);\n",
        "    }\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(dA);\n",
        "    cudaFree(dB);\n",
        "    cudaFree(dX);\n",
        "    cudaFree(dipiv);\n",
        "    cusolverDnDestroy(handle);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver -arch=sm_35 -Wno-deprecated-gpu-targets prog.cu"
      ],
      "metadata": {
        "id": "pHtMWhcpq_0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5MRchpZrE-a",
        "outputId": "b175e3a9-c727-42ab-a7f5-b53194daba8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 74000ng 32603 0\n",
            "Error solving linear system: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prog.cu\n",
        "// This program computes a simple version of matrix multiplication\n",
        "// By: Nick from CoffeeBeforeArch\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cassert>\n",
        "#include <cstdlib>\n",
        "#include <functional>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "using std::cout;\n",
        "using std::generate;\n",
        "using std::vector;\n",
        "\n",
        "__global__ void matrixMul(const int *a, const int *b, int *c, int N) {\n",
        "  // Compute each thread's global row and column index\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  // Iterate over row, and down column\n",
        "  c[row * N + col] = 0;\n",
        "  for (int k = 0; k < N; k++) {\n",
        "    // Accumulate results for a single element\n",
        "    c[row * N + col] += a[row * N + k] * b[k * N + col];\n",
        "  }\n",
        "}\n",
        "\n",
        "// Check result on the CPU\n",
        "void verify_result(vector<int> &a, vector<int> &b, vector<int> &c, int N) {\n",
        "  // For every row...\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    // For every column...\n",
        "    for (int j = 0; j < N; j++) {\n",
        "      // For every element in the row-column pair\n",
        "      int tmp = 0;\n",
        "      for (int k = 0; k < N; k++) {\n",
        "        // Accumulate the partial results\n",
        "        tmp += a[i * N + k] * b[k * N + j];\n",
        "      }\n",
        "\n",
        "      // Check against the CPU result\n",
        "      assert(tmp == c[i * N + j]);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  // Matrix size of 1024 x 1024;\n",
        "  int N = 1 << 10;\n",
        "\n",
        "  // Size (in bytes) of matrix\n",
        "  size_t bytes = N * N * sizeof(int);\n",
        "\n",
        "  // Host vectors\n",
        "  vector<int> h_a(N * N);\n",
        "  vector<int> h_b(N * N);\n",
        "  vector<int> h_c(N * N);\n",
        "\n",
        "  // Initialize matrices\n",
        "  generate(h_a.begin(), h_a.end(), []() { return rand() % 100; });\n",
        "  generate(h_b.begin(), h_b.end(), []() { return rand() % 100; });\n",
        "\n",
        "  // Allocate device memory\n",
        "  int *d_a, *d_b, *d_c;\n",
        "  cudaMalloc(&d_a, bytes);\n",
        "  cudaMalloc(&d_b, bytes);\n",
        "  cudaMalloc(&d_c, bytes);\n",
        "\n",
        "  // Copy data to the device\n",
        "  cudaMemcpy(d_a, h_a.data(), bytes, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, h_b.data(), bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Threads per CTA dimension\n",
        "  int THREADS = 32;\n",
        "\n",
        "  // Blocks per grid dimension (assumes THREADS divides N evenly)\n",
        "  int BLOCKS = N / THREADS;\n",
        "\n",
        "  // Use dim3 structs for block  and grid dimensions\n",
        "  dim3 threads(THREADS, THREADS);\n",
        "  dim3 blocks(BLOCKS, BLOCKS);\n",
        "\n",
        "  // Launch kernel\n",
        "  matrixMul<<<blocks, threads>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "  // Copy back to the host\n",
        "  cudaMemcpy(h_c.data(), d_c, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  // Check result\n",
        "  verify_result(h_a, h_b, h_c, N);\n",
        "\n",
        "  cout << \"COMPLETED SUCCESSFULLY\\n\";\n",
        "\n",
        "  // Free memory on device\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzuokzqyrF8a",
        "outputId": "2f691564-4035-4a10-f780-b1a4e8f2bd00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prog.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver -arch=sm_35 -Wno-deprecated-gpu-targets prog.cu\n",
        "!./a.out"
      ],
      "metadata": {
        "id": "v3JTPq4YuX2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pK5izBYVwNtb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}